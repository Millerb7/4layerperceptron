{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Four: Multi-Layer Perceptron\n",
    "\n",
    "Code by Miller Boyd\n",
    "\n",
    "## Introduction\n",
    "In this lab, you will compare the performance of multi-layer perceptrons with your own implementations. This project is a crucial component of the course, constituting 10% of the final grade. Teams are required to submit a comprehensive report in a Jupyter notebook format, including all code, visualizations, and narratives. For visualizations that cannot be directly embedded, include screenshots. Ensure the results are reproducible using the submitted notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "You will employ the US Census data for this assignment, specifically chosen by the instructor. This dataset is available on Kaggle and can also be downloaded from [Dropbox](https://www.dropbox.com/s/bf7i7qjftk7cmzq/acs2017_census_tract_data.csv?dl=0). The classification task involves predicting the child poverty rate across different tracts, requiring you to convert this into a four-level classification task by quantizing the variable of interest.\n",
    "\n",
    "Found in file: acs2017_census_tract_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, Split, and Balance (1.5 points)\n",
    "- **[0.5 points]** Load the data into a pandas DataFrame, remove missing data, encode strings as integers, and decide on the inclusion of the \"county\" variable with justification.\n",
    "- **[0.5 points]** Balance the dataset ensuring an equal number of instances across classes. Explain your chosen method for balancing and whether it applies to both training and testing sets.\n",
    "- **[0.5 points]** Split the dataset into an 80/20 train/test ratio, aiming for equal classification performance across classes. Only one-hot encode the target at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = 'acs2017_census_tract_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Remove missing data\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode string data as integers\n",
    "# Assuming 'county' is a string variable among others. Adjust as necessary.\n",
    "le = LabelEncoder()\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col] = le.fit_transform(df[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of            TractId  State  County  TotalPop   Men  Women  Hispanic  White  \\\n",
       "0       1001020100      0      89      1845   899    946       2.4   86.3   \n",
       "1       1001020200      0      89      2172  1167   1005       1.1   41.6   \n",
       "2       1001020300      0      89      3385  1533   1852       8.0   61.4   \n",
       "3       1001020400      0      89      4267  2001   2266       9.6   80.3   \n",
       "4       1001020500      0      89      9965  5054   4911       0.9   77.5   \n",
       "...            ...    ...     ...       ...   ...    ...       ...    ...   \n",
       "73996  72153750501     39    1938      6011  3035   2976      99.7    0.3   \n",
       "73997  72153750502     39    1938      2342   959   1383      99.1    0.9   \n",
       "73998  72153750503     39    1938      2218  1001   1217      99.5    0.2   \n",
       "73999  72153750601     39    1938      4380  1964   2416     100.0    0.0   \n",
       "74000  72153750602     39    1938      3001  1343   1658      99.2    0.8   \n",
       "\n",
       "       Black  Native  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  \\\n",
       "0        5.2     0.0  ...   0.5          0.0         2.1         24.5   \n",
       "1       54.5     0.0  ...   0.0          0.5         0.0         22.2   \n",
       "2       26.5     0.6  ...   1.0          0.8         1.5         23.1   \n",
       "3        7.1     0.5  ...   1.5          2.9         2.1         25.9   \n",
       "4       16.4     0.0  ...   0.8          0.3         0.7         21.0   \n",
       "...      ...     ...  ...   ...          ...         ...          ...   \n",
       "73996    0.0     0.0  ...   0.5          0.0         3.6         26.9   \n",
       "73997    0.0     0.0  ...   0.0          0.0         1.3         25.3   \n",
       "73998    0.0     0.0  ...   3.4          0.0         3.4         23.5   \n",
       "73999    0.0     0.0  ...   0.0          0.0         0.0         24.1   \n",
       "74000    0.0     0.0  ...   4.9          0.0         8.9         21.6   \n",
       "\n",
       "       Employed  PrivateWork  PublicWork  SelfEmployed  FamilyWork  \\\n",
       "0           881         74.2        21.2           4.5         0.0   \n",
       "1           852         75.9        15.0           9.0         0.0   \n",
       "2          1482         73.3        21.1           4.8         0.7   \n",
       "3          1849         75.8        19.7           4.5         0.0   \n",
       "4          4787         71.4        24.1           4.5         0.0   \n",
       "...         ...          ...         ...           ...         ...   \n",
       "73996      1576         59.2        33.8           7.0         0.0   \n",
       "73997       666         58.4        35.4           6.2         0.0   \n",
       "73998       560         57.5        34.5           8.0         0.0   \n",
       "73999      1062         67.7        30.4           1.9         0.0   \n",
       "74000       759         75.9        19.1           5.0         0.0   \n",
       "\n",
       "       Unemployment  \n",
       "0               4.6  \n",
       "1               3.4  \n",
       "2               4.7  \n",
       "3               6.1  \n",
       "4               2.3  \n",
       "...             ...  \n",
       "73996          20.8  \n",
       "73997          26.3  \n",
       "73998          23.0  \n",
       "73999          29.5  \n",
       "74000          17.9  \n",
       "\n",
       "[72718 rows x 37 columns]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decide on the inclusion of the \"county\" variable\n",
    "# This is a placeholder for your analysis on whether to keep or remove the 'county' variable.\n",
    "# Example decision: Remove 'county' if it was encoded above\n",
    "# df = df.drop(columns=['county'])\n",
    "df.columns\n",
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before balancing: ChildPoverty_Quantized\n",
      "0    18229\n",
      "1    18171\n",
      "3    18170\n",
      "2    18148\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wnd\\AppData\\Local\\Temp\\ipykernel_96936\\4116637881.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_balanced = df.groupby('ChildPoverty_Quantized').apply(lambda x: x.sample(min_class_size)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Balance the dataset\n",
    "# First, quantize the 'ChildPoverty' variable into 4 classes\n",
    "df['ChildPoverty_Quantized'] = pd.qcut(df['ChildPoverty'], 4, labels=False)\n",
    "\n",
    "# Count the number of instances in each class to decide on balancing method\n",
    "class_counts = df['ChildPoverty_Quantized'].value_counts()\n",
    "print(\"Class distribution before balancing:\", class_counts)\n",
    "\n",
    "# Assuming a simple undersampling strategy for balancing for demonstration\n",
    "# Ideally, you might want to explore more sophisticated methods depending on class distribution\n",
    "min_class_size = class_counts.min()\n",
    "df_balanced = df.groupby('ChildPoverty_Quantized').apply(lambda x: x.sample(min_class_size)).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 58073\n",
      "Testing set size: 14519\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into 80% training and 20% testing\n",
    "# Assuming 'ChildPoverty_Quantized' is the target variable\n",
    "X = df_balanced.drop(['ChildPoverty_Quantized'], axis=1)\n",
    "y = df_balanced['ChildPoverty_Quantized']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Testing set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pre-processing and Initial Modeling (2.5 points)\n",
    "- [.5 points] Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit as sigmoid\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class TwoLayerPerceptronBase:\n",
    "    def __init__(self, n_hidden=30, C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Glorot initialization\n",
    "        limit1 = np.sqrt(6 / (self.n_features_ + 1 + self.n_hidden))\n",
    "        W1 = np.random.uniform(-limit1, limit1, size=(self.n_hidden, self.n_features_ + 1))\n",
    "        \n",
    "        limit2 = np.sqrt(6 / (self.n_hidden + 1 + self.n_output_))\n",
    "        W2 = np.random.uniform(-limit2, limit2, size=(self.n_output_, self.n_hidden + 1))\n",
    "        return W1, W2\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        if how == 'column':\n",
    "            X_new = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        elif how == 'row':\n",
    "            X_new = np.vstack((np.ones((1, X.shape[1])), X))\n",
    "        return X_new\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        return (lambda_/2.0) * (np.sum(W1[:, 1:] ** 2) + np.sum(W2[:, 1:] ** 2))\n",
    "\n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        A1 = self._add_bias_unit(X, how='column')\n",
    "        Z1 = W1 @ A1.T\n",
    "        A2 = sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "\n",
    "    def predict(self, X):\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "\n",
    "class TLPMiniBatchCrossEntropy(TwoLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.0, shuffle=True, minibatches=1, **kwds):\n",
    "        super().__init__(**kwds)\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "\n",
    "    def _cost(self, A3, Y_enc, W1, W2):\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc * np.log(A3) + (1 - Y_enc) * np.log(1 - A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        V2 = (A3 - Y_enc)\n",
    "        V1 = A2 * (1 - A2) * (W2.T @ V2)\n",
    "        grad2 = V2 @ A2.T\n",
    "        grad1 = V1[1:, :] @ A1.T\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "        return grad1, grad2\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        self.score_.append(accuracy_score(y_data, self.predict(X_data)))\n",
    "        if XY_test is not None:\n",
    "            X_test, y_test = XY_test\n",
    "            self.val_score_ = [accuracy_score(y_test, self.predict(X_test))]\n",
    "        for i in range(self.epochs):\n",
    "            eta = self.eta / (1 + self.decrease_const * i)\n",
    "            if print_progress and (i + 1) % print_progress == 0:\n",
    "                print(f'\\rEpoch: {i+1}/{self.epochs}', end='')\n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx], Y_enc[:, idx], y_data[idx]\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx], self.W1, self.W2)\n",
    "                cost = self._cost(A3, Y_enc[:, idx], self.W1, self.W2)\n",
    "                self.cost_.append(cost)\n",
    "                grad1, grad2 = self._get_gradient(A1, A2, A3, Z1, Z2, Y_enc[:, idx], self.W1, self.W2)\n",
    "                self.W1 -= eta * grad1\n",
    "                self.W2 -= eta * grad2\n",
    "            self.score_.append(accuracy_score(y_data, self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                self.val_score_.append(accuracy_score(y_test, self.predict(X_test)))\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([33757, 34565, 56880, 57742, 33027, 38594, 50984, 36810,  3194,  1422,\\n       ...\\n       47476, 25737, 53153, 38751, 51226, 16661, 37022,  8187, 40003,  9338],\\n      dtype='int32', length=58073)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m tlp \u001b[38;5;241m=\u001b[39m TLPMiniBatchCrossEntropy(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, minibatches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, n_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Fit the model to the training data\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Plotting the average cost over epochs\u001b[39;00m\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tlp\u001b[38;5;241m.\u001b[39mcost_)), tlp\u001b[38;5;241m.\u001b[39mcost_, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 95\u001b[0m, in \u001b[0;36mTLPMiniBatchCrossEntropy.fit\u001b[1;34m(self, X, y, print_progress, XY_test)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle:\n\u001b[0;32m     94\u001b[0m     idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(y_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 95\u001b[0m     X_data, Y_enc, y_data \u001b[38;5;241m=\u001b[39m \u001b[43mX_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, Y_enc[:, idx], y_data[idx]\n\u001b[0;32m     96\u001b[0m mini \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray_split(\u001b[38;5;28mrange\u001b[39m(y_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminibatches)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m mini:\n",
      "File \u001b[1;32mc:\\Users\\wnd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\wnd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wnd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index([33757, 34565, 56880, 57742, 33027, 38594, 50984, 36810,  3194,  1422,\\n       ...\\n       47476, 25737, 53153, 38751, 51226, 16661, 37022,  8187, 40003,  9338],\\n      dtype='int32', length=58073)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a dataset X and labels y\n",
    "# Split your dataset into training and test sets\n",
    "# For simplicity, let's assume X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "# Instantiate the model\n",
    "tlp = TLPMiniBatchCrossEntropy(epochs=100, eta=0.001, minibatches=50, n_hidden=30, random_state=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "tlp.fit(X_train, y_train, print_progress=True)\n",
    "\n",
    "# Plotting the average cost over epochs\n",
    "plt.plot(range(len(tlp.cost_)), tlp.cost_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Cost')\n",
    "plt.title('Cost vs. Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "test_accuracy = accuracy_score(y_test, tlp.predict(X_test))\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [.5 points] Now (1) normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [.5 points] Now(1) normalize the continuous numeric feature data AND (2) one hot encode the categorical data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1 points] Compare the performance of the three models you just trained. Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances.  Use one-hot encoding and normalization on the dataset for the remainder of this lab assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Modeling (5 points)\n",
    "- **[1 point]** Extend the perceptron model to include a third layer, incorporating gradient magnitude tracking for each layer per epoch. Quantify performance and graph gradient magnitudes.\n",
    "- **[1 point]** Add a fourth layer to the model, repeating the performance quantification and gradient magnitude tracking.\n",
    "- **[1 point]** Introduce a fifth layer, continuing with performance quantification and gradient tracking.\n",
    "- **[2 points]** Implement an adaptive learning technique (excluding AdaM) for the five-layer network. Discuss your choice of technique, compare model performances with and without the adaptive strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptional Work (1 point)\n",
    "- **5000 level students:** You are encouraged to explore additional analyses.\n",
    "- **7000 level students (required):** Implement AdaM in the five-layer neural network and compare its performance with other models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
